{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª set up your environment\n",
    "\n",
    "import `Processor` to restructure and clean your data in a way which allows for rigorous, accurate tuning according to your goals\n",
    "\n",
    "###### ‚ÑπÔ∏è your source data must be csv/json/parquet path OR a dataframe object and contain **at least** one column with plaintext\n",
    "\n",
    "###### ‚ö°Ô∏èüß≤ assuming for each transformation that you have the step-wise data ready to go, this block is all you need to initialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.filings import Processor\n",
    "source_data_file = \"./raw/kb_export_clean.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìë create sentences from plaintext\n",
    "\n",
    "we set an an input file with a `Processor` class to create `filings` out of our data\n",
    "\n",
    "###### ‚ÑπÔ∏è you do not need an `id` column, we will make a document-level, integer-wise one for each of your sentences automatically, but keep this in mind for re-indexing your embeddings back to sentences or documents!\n",
    "\n",
    "###### ‚ÑπÔ∏è then we load the specific raw data file into memory! this is to protect your sequential workflow requirements but if you need to conserve resources during steps, look into the `.unload()` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: loaded - ./raw/kb_export_clean.csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "filings = Processor()\n",
    "filings.load(source_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü•≥ great! let's process our data, _fast_\n",
    "\n",
    "‚ö°Ô∏èüß≤ first we extract sentences for our embedding model to get initial scores and examples which we call `filings`\n",
    "\n",
    "###### ‚ÑπÔ∏è don't forget to declare your plaintext column's name! we do not persist this between objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m‚òïÔ∏è WAIT: get coffee or tea - 65822 processing...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65822/65822 [01:57<00:00, 562.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: saved - ./data/filings.parquet\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "filings.export_as_sentences('./data/filings.parquet','clean','file', nlp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next-generation data communication & processing with NATS\n",
    "\n",
    "Suppose you have a massive volume of data and this workload needs to be distributed? \n",
    "Magnet supports [NATS](https://nats.io) in a clever abstraction for sharing data transformation receipts, and the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.ic import field\n",
    "charge = field.Charge(\"my-user:T0pS3cr3t@nats-cluster.workspace.svc.cluster.local\") # your NATS server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà‚òÅÔ∏è upload to S3 on AWS\n",
    "\n",
    "how easy! not much to say here other than this will upload your entire processed data folder when done (we assume you got your original data from somewhere!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.utils import Utils\n",
    "\n",
    "Utils().upload_to_s3(\n",
    "    './data/'\n",
    "    , ('AWS_CLIENT_KEY', 'AWS_SECRET_KEY')\n",
    "    , 'bucket_name'\n",
    "    , 'finetuning_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
