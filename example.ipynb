{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª set up your environment\n",
    "\n",
    "import `Processor` to restructure and clean your data in a way which allows for rigorous, accurate tuning according to your goals\n",
    "\n",
    "###### ‚ÑπÔ∏è your source data must be csv/json/parquet path OR a dataframe object and contain **at least** one column with plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.filings import Processor\n",
    "source_data_file = \"./raw/kb_export_clean.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìë create chunks from documents\n",
    "\n",
    "we set an an input file with a `Processor` class to create `filings` out of our data\n",
    "\n",
    "###### ‚ÑπÔ∏è you do not need an `id` column, we will make a document-level, integer-wise one for each of your sentences automatically, but keep this in mind for re-indexing your embeddings back to sentences or documents!\n",
    "\n",
    "###### ‚ÑπÔ∏è then we load the raw data file into memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings = Processor()\n",
    "filings.load(source_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü•≥ great! let's process our data, _fast_\n",
    "\n",
    "‚ö°Ô∏èüß≤ first we extract sentences for our embedding model to get initial scores and examples which we call `filings`\n",
    "\n",
    "###### ‚ÑπÔ∏è don't forget to declare your plaintext column's name! we do not persist this between objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await filings.process('./data/filings.parquet','clean','file', nlp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ∞Ô∏è next-generation data communication & processing with NATS\n",
    "\n",
    "##### ‚ÑπÔ∏è suppose you have a massive volume of data and this workload needs to be distributed \n",
    "\n",
    "###### üì° by setting up a `magnet` Resonator with an existing NATS server, you can stream your data to another node rapidly\n",
    "\n",
    "##### `magnet` supports [NATS](https://nats.io) in a clever abstraction for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.ic import field\n",
    "nats_cluster = field.Charge(\"my-user:T0pS3cr3t@192.168.2.69\") # your NATS cluster hostname & basic auth\n",
    "clustered_filings = Processor(field=nats_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üíæ load your data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_filings.load(source_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üì° at another workstation, run the following to receive processed data\n",
    "\n",
    "```python\n",
    "from magnet.ic import field\n",
    "reso = field.Resonator(\"my-user:T0pS3cr3t@192.168.2.69\")\n",
    "def handle_payload(payload):\n",
    "    print(payload.text[0:10])\n",
    "    print(payload.document)\n",
    "await reso.on(cb=handle_payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üì° export, but this time your data will also stream to a different node running `magnet` as it's being written to the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await clustered_filings.process('./data/filings.parquet','clean','file', nlp=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü´• delete the persistent stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.ic import field\n",
    "charge = field.Charge(\"my-user:T0pS3cr3t@192.168.2.69\")\n",
    "await charge.on()\n",
    "await charge.emp()\n",
    "await charge.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßÆ Vector DB implementation\n",
    "\n",
    "###### `magnet` currently supports `milvus` and with it you can fully manage your vector DB in the context of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.ron import embed\n",
    "config = {\n",
    "    \"MILVUS_URI\": \"192.168.2.69\"\n",
    "    , \"DIMENSION\": 1024\n",
    "    , \"INDEX\": \"test_collection\"\n",
    "    , \"MODEL\": \"BAAI/bge-large-en-v1.5\"\n",
    "}\n",
    "\n",
    "embedder = embed.Embedder(config, create=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîé including search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = [\n",
    "    {'text':'Testing','document':'document0'}\n",
    "    , {'text':'test','document':'document1'}\n",
    "    , {'text':'vector','document':'document2'}\n",
    "]\n",
    "[embedder.embed_and_store(x) for x in json]\n",
    "results = embedder.search({'text':'test'})\n",
    "embedder.delete()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà‚òÅÔ∏è upload to S3\n",
    "\n",
    "upload your entire processed data folder when done (we assume you got your original data from somewhere!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.utils import Utils\n",
    "\n",
    "Utils().upload_to_s3(\n",
    "    './data/'\n",
    "    , ('AWS_CLIENT_KEY', 'AWS_SECRET_KEY')\n",
    "    , 'bucket_name'\n",
    "    , 'finetuning_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
