{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª set up your environment\n",
    "\n",
    "import `Processor` to restructure and clean your data in a way which allows for rigorous, accurate tuning according to your goals\n",
    "\n",
    "###### ‚ÑπÔ∏è your source data must be csv/json/parquet path OR a dataframe object and contain **at least** one column with plaintext\n",
    "\n",
    "###### ‚ö°Ô∏èüß≤ assuming for each transformation that you have the step-wise data ready to go, this block is all you need to initialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 19:41:31.644208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-23 19:41:32.247727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-23 19:41:32.679424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-23 19:41:32.680198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-23 19:41:32.680310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "from magnet.filings import Processor\n",
    "source_data_file = \"./raw/kb_export_clean.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìë create sentences from plaintext\n",
    "\n",
    "we set an an input file with a `Processor` class to create `filings` out of our data\n",
    "\n",
    "###### ‚ÑπÔ∏è you do not need an `id` column, we will make a document-level, integer-wise one for each of your sentences automatically, but keep this in mind for re-indexing your embeddings back to sentences or documents!\n",
    "\n",
    "###### ‚ÑπÔ∏è then we load the specific raw data file into memory! this is to protect your sequential workflow requirements but if you need to conserve resources during steps, look into the `.unload()` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: loaded - ./raw/kb_export_clean.parquet\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "filings = Processor()\n",
    "filings.load(source_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü•≥ great! let's process our data, _fast_\n",
    "\n",
    "‚ö°Ô∏èüß≤ first we extract sentences for our embedding model to get initial scores and examples which we call `filings`\n",
    "\n",
    "###### ‚ÑπÔ∏è don't forget to declare your plaintext column's name! we do not persist this between objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m‚òïÔ∏è WAIT: get coffee or tea - 13103 processing...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 120/13103 [00:02<03:34, 60.47it/s]"
     ]
    }
   ],
   "source": [
    "filings.export_as_sentences('./data/sentences.parquet','clean','id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßÆ indexing data \n",
    "\n",
    "in `magnet.ize`, we have different submodules responsible for different parts of building our \"data field\" of vectors.\n",
    "\n",
    "import `charge` to create a \"Pole\" and index your sentences to it using your embedding model.\n",
    "you can think of the vectors as electrons which belong to charged particles in a magnetic pole, or you can ignore the metaphor completely!\n",
    "\n",
    "then we save our embeddings to re-use later and upload/share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m üåä SUCCESS: embedded sentence 930554\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 930555/930555 [4:12:35<00:00, 61.40it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m‚òïÔ∏è WAIT: indexing 930555 objects\u001b[0m\n",
      "\u001b[92müåä SUCCESS: index created\u001b[0m\n",
      "\u001b[92müåä SUCCESS: embeddings saved to ./data/sentence_embeddings.index\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from magnet.ize import charge\n",
    "import pandas as pd\n",
    "\n",
    "sentences = pd.read_parquet('./data/sentences.parquet')\n",
    "charge = charge.Pole()\n",
    "charge.index_document_embeddings(df=sentences)\n",
    "charge.save_embeddings('./data/sentence_embeddings.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü§Ø amazing and easy! \n",
    "\n",
    "let's now use these scores to create positive and negative training examples. here we search, re-index back to our sentences in plaintext with their IDs, and return the results as positive and negative examples\n",
    "\n",
    "in `magnet.ron`, we have functions to create tuned versions of your `Pole` after charging with sentences from your knowledge base. You can think of this as energy interacting with a prism and orienting certain features in certain angles (you'll understand this more as we dive into embedding model use-cases!). In our example we will do positively and negatively correlated samples for finetuning on a `similarity` task from our vast volume of data (energy).\n",
    "\n",
    "###### ‚ÑπÔ∏è we use a random distribution the size of your `split` argument from your data! you can also change the amount of your index is allowed to pass through your prism with `k` argument\n",
    "\n",
    "Let's create our `Prism` class from `tune`\n",
    "\n",
    "###### ‚ÑπÔ∏è check out a given model's documentation for more on how to format your data, ours defaults to the format required to tune `BAAI/bge-large-en-v1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: loaded -                                                  sentences     id\n",
      "0        Joseph Stilwell disclaims beneficial ownership...      0\n",
      "1        Joseph Stilwell disclaims beneficial ownership...      0\n",
      "2        Joseph Stilwell disclaims beneficial ownership...      0\n",
      "3        ÔªøownershipDocument: schemaVersion: X0508docume...      0\n",
      "4        ip: isDirector: 0isOfficer: 0isTenPercentOwner...      0\n",
      "...                                                    ...    ...\n",
      "1248817  Exhibit Number Exhibit Description 1.1 Form of...  13102\n",
      "1248818   Pickett dated May 1, 2022 10.3+ Consulting Ag...  13102\n",
      "1248819  cates a management contract or any compensator...  13102\n",
      "1248820  By: /s/ Tim Pickett Tim Pickett Chief Executiv...  13102\n",
      "1248821  d all documents in connection therewith, with ...  13102\n",
      "\n",
      "[1248822 rows x 2 columns]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/78051 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: index loaded - ./data/sentence_embeddings.index\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m üìä ‚£Ω: processed  - \"44 Nonfarm animal careta kers...................................................................................................... 163 66.3Gaming services workers........................................................................................................ .\"\u001b[0m:   0%|          | 10/78051 [00:25<46:49:03,  2.16s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[95m ‚ù§Ô∏è ‚¢ø: processed  - \"Any amendment, repeal or modification of this Article IX, or the adoption of any provision of these Bylawsinconsistent with this Article IX, whether by action of the Board of Directors or the shareholders of the Corporation, shall not applyto or adversely affect any right or protection of a director of the Corporation existing at the time of such amendment, repeal, modificationor adoption.\"\u001b[0m:   3%|‚ñé         | 2224/78051 [1:17:34<44:04:42,  2.09s/it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/dylan/AI/magnet/example.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.69/media/dylan/AI/magnet/example.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m kb_prepper \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mPrism()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.69/media/dylan/AI/magnet/example.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m kb_prepper\u001b[39m.\u001b[39mload(sentences)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.69/media/dylan/AI/magnet/example.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m kb_prepper\u001b[39m.\u001b[39;49mgenerate_training_data(out_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/sentence_embeddings.index\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/media/dylan/AI/magnet/magnet/ron/tune.py:70\u001b[0m, in \u001b[0;36mPrism.generate_training_data\u001b[0;34m(self, out_dir, split, k, index, num_pos, num_neg, index_to_gpu)\u001b[0m\n\u001b[1;32m     68\u001b[0m kb_index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(data))\n\u001b[1;32m     69\u001b[0m q \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[kb_index]\n\u001b[0;32m---> 70\u001b[0m embeddings \u001b[39m=\u001b[39m pole\u001b[39m.\u001b[39;49msearch_document_embeddings(q, k\u001b[39m=\u001b[39;49mk, df\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf)\n\u001b[1;32m     71\u001b[0m pos_results \u001b[39m=\u001b[39m embeddings[\u001b[39m0\u001b[39m:num_pos]\n\u001b[1;32m     72\u001b[0m neg_results \u001b[39m=\u001b[39m embeddings[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m:num_neg]\n",
      "File \u001b[0;32m/media/dylan/AI/magnet/magnet/ize/charge.py:52\u001b[0m, in \u001b[0;36mPole.search_document_embeddings\u001b[0;34m(self, q, k, df)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_document_embeddings\u001b[39m(\u001b[39mself\u001b[39m, q: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m, df: pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[1;32m     53\u001b[0m         xq \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode([q])\n\u001b[1;32m     54\u001b[0m         D, I  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_index\u001b[39m.\u001b[39msearch(xq, k)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[1;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[1;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[1;32m    136\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39;49minput_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m     28\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2876\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2875\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2876\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2878\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2879\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:892\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m    891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m BertEmbeddings(config)\n\u001b[0;32m--> 892\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BertEncoder(config)\n\u001b[1;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m BertPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:560\u001b[0m, in \u001b[0;36mBertEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:560\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:483\u001b[0m, in \u001b[0;36mBertLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m BertAttention(config, position_embedding_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate \u001b[39m=\u001b[39m BertIntermediate(config)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m BertOutput(config)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:459\u001b[0m, in \u001b[0;36mBertOutput.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    458\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mintermediate_size, config\u001b[39m.\u001b[39;49mhidden_size)\n\u001b[1;32m    460\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_eps)\n\u001b[1;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mhidden_dropout_prob)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from magnet.ron import tune\n",
    "import pandas as pd\n",
    "\n",
    "sentences = pd.read_parquet('./data/sentences.parquet')\n",
    "task = 'similarity'\n",
    "kb_prepper = tune.Prism()\n",
    "kb_prepper.load(sentences)\n",
    "kb_prepper.generate_training_data(out_dir='./data/', index='./data/sentence_embeddings.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà‚òÅÔ∏è upload to S3 on AWS\n",
    "\n",
    "how easy! not much to say here other than this will upload your entire processed data folder when done (we assume you got your original data from somewhere!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.utils import Utils\n",
    "\n",
    "Utils().upload_to_s3(\n",
    "    './data/'\n",
    "    , ('AWS_CLIENT_KEY', 'AWS_SECRET_KEY')\n",
    "    , 'bucket_name'\n",
    "    , 'finetuning_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
